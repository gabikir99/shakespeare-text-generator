{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae399dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gauriel\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, what matters in the world is the world we live in, and if you want to live in an environment that's more like the world you live in, you should be more aware of this,\" she said.\n",
      "\n",
      "\"It's about how\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",  # or any other model you want to use\n",
    ")\n",
    "\n",
    "# 2. (Optional) Fix the seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# 3. Generate\n",
    "output = generator(\n",
    "    \"To be or not to be\",   # your prompt\n",
    "    max_new_tokens=50,      # ONLY counts the new tokens\n",
    "    do_sample=True,         # sampling instead of greedy\n",
    "    top_k=50,               # consider the 50 most likely next tokens\n",
    "    temperature=0.7,        # lower → more conservative; higher → more creative\n",
    "    num_return_sequences=1  # how many completions you want\n",
    ")\n",
    "\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7922ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 26265.29 examples/s]\n",
      "                                     \n",
      "100%|██████████| 3/3 [01:02<00:00, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 62.0458, 'train_samples_per_second': 1.112, 'train_steps_per_second': 0.048, 'train_loss': 4.524337450663249, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=4.524337450663249, metrics={'train_runtime': 62.0458, 'train_samples_per_second': 1.112, 'train_steps_per_second': 0.048, 'total_flos': 12019433472000.0, 'train_loss': 4.524337450663249, 'epoch': 2.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# 1. Load your text file as a Dataset\n",
    "ds = load_dataset(\"text\", data_files={\"train\": \"first_1000_lines.txt\"})\n",
    "\n",
    "# 2. Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model     = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 3. Tokenize & group into blocks\n",
    "def tokenize_and_group(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], return_special_tokens_mask=False)\n",
    "    all_ids = sum(tokens[\"input_ids\"], [])\n",
    "    # chunk into non-overlapping blocks of size block_size\n",
    "    block_size = 512\n",
    "    input_ids = [\n",
    "        all_ids[i : i + block_size]\n",
    "        for i in range(0, len(all_ids) - block_size + 1, block_size)\n",
    "    ]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [[1]*block_size]*len(input_ids)}\n",
    "\n",
    "tokenized = ds[\"train\"].map(\n",
    "    tokenize_and_group,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# 4. Data collator for causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# 5. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    fp16=False,                # if you have a GPU with half-precision\n",
    ")\n",
    "\n",
    "# 6. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 7. Fine-tune!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17d55f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2-finetuned\\\\tokenizer_config.json',\n",
       " 'gpt2-finetuned\\\\special_tokens_map.json',\n",
       " 'gpt2-finetuned\\\\vocab.json',\n",
       " 'gpt2-finetuned\\\\merges.txt',\n",
       " 'gpt2-finetuned\\\\added_tokens.json',\n",
       " 'gpt2-finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"gpt2-finetuned\")            # writes pytorch_model.bin here\n",
    "tokenizer.save_pretrained(\"gpt2-finetuned\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdaca2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be, that is the question and the answer. For it is possible that all things, whether real or unreal, are possible, that is the issue. It is also possible that things, other than real things, are possible. Nothing is possible in order to exist. The question is whether it is possible to exist or not, not even to exist. There is no doubt that it is possible. No one can deny that we are all living beings. But that cannot be the only reality. That is the question. We can\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. Create a text-generation pipeline pointing at your checkpoint\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-finetuned\",   # your fine-tuned model directory\n",
    "    tokenizer=\"gpt2-finetuned\",           # or \"./gpt2-finetuned\" if you saved tokenizer too\n",
    "    device=-1,  # -1 for CPU, 0 for GPU\n",
    ")\n",
    "\n",
    "# 2. Run on an unseen prompt\n",
    "prompt = \"to be or not to be, that is the question\"\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=40,\n",
    "    temperature=0.8,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
